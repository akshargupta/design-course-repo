{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Lesson 1: Vectors as Meaning\n",
    "\n",
    "**Course:** Foundations of Attention  \n",
    "**Module:** The Geometry of Intelligence  \n",
    "**Learning Objective:** Define high-dimensional vector spaces and their relevance to semantic meaning (Concept)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: The Bridge Between Language and Math\n",
    "\n",
    "How do computers understand language?\n",
    "\n",
    "The answer is: **they don't**‚Äîat least not the way humans do. But they can understand **geometry**. And that's the key insight that powers every modern AI system:\n",
    "\n",
    "> **We can represent meaning as geometry.**\n",
    "\n",
    "In this lesson, you'll learn how words become vectors, and why high-dimensional space is the secret to encoding semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Vector?\n",
    "\n",
    "A **vector** is simply a list of numbers. That's it.\n",
    "\n",
    "```python\n",
    "v = [0.5, 0.3, -0.2]\n",
    "```\n",
    "\n",
    "But here's the powerful part: we can interpret those numbers as **coordinates in space**.\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "A vector can be visualized as:\n",
    "1. **A point** in space (the coordinates)\n",
    "2. **An arrow** from the origin to that point (direction and magnitude)\n",
    "\n",
    "For example:\n",
    "- `[2, 3]` is a point in 2D space\n",
    "- `[1, 2, 3]` is a point in 3D space\n",
    "- `[0.1, 0.2, 0.3, ..., 0.768]` is a point in 768D space\n",
    "\n",
    "### Why Vectors Matter for AI\n",
    "\n",
    "In machine learning, we use vectors to represent **anything**:\n",
    "- A word: `\"cat\" ‚Üí [0.12, -0.45, 0.78, ...]`\n",
    "- An image: `cat.jpg ‚Üí [0.23, 0.67, -0.12, ...]`\n",
    "- A user: `user_123 ‚Üí [0.34, 0.89, -0.56, ...]`\n",
    "\n",
    "Once everything is a vector, we can use **geometry** to measure relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Vector Space?\n",
    "\n",
    "A **vector space** is the mathematical \"universe\" where vectors live. Think of it as:\n",
    "- A canvas where every possible vector has a unique location\n",
    "- A coordinate system with axes (dimensions)\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "The **dimension** of a vector space is how many numbers you need to specify a location:\n",
    "\n",
    "| Space | Dimensions | Example Vector |\n",
    "|-------|------------|----------------|\n",
    "| 2D (plane) | 2 | `[3, 4]` |\n",
    "| 3D (physical space) | 3 | `[1, 2, 5]` |\n",
    "| 768D (GPT-2 embeddings) | 768 | `[0.12, -0.34, ..., 0.56]` |\n",
    "| 1536D (GPT-3 embeddings) | 1536 | Even more numbers! |\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "In a vector space, you can:\n",
    "1. **Add vectors**: `[1, 2] + [3, 4] = [4, 6]`\n",
    "2. **Scale vectors**: `2 √ó [1, 2] = [2, 4]`\n",
    "3. **Measure distance** between vectors\n",
    "4. **Measure direction** (which we'll explore in the next lesson)\n",
    "\n",
    "These operations let us do math on meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Words to Vectors: The Embedding\n",
    "\n",
    "### The Problem: Computers Can't Read\n",
    "\n",
    "Computers see text as raw bytes:\n",
    "```\n",
    "\"cat\" = [99, 97, 116]  # ASCII codes\n",
    "```\n",
    "\n",
    "There's no semantic meaning in these numbers. \"cat\" and \"dog\" are just different byte sequences.\n",
    "\n",
    "### The Old Solution: One-Hot Encoding\n",
    "\n",
    "Historically, we represented each word as a sparse vector:\n",
    "\n",
    "```\n",
    "Vocabulary: [\"cat\", \"dog\", \"king\", \"queen\", \"apple\"]\n",
    "\n",
    "cat   = [1, 0, 0, 0, 0]\n",
    "dog   = [0, 1, 0, 0, 0]\n",
    "king  = [0, 0, 1, 0, 0]\n",
    "queen = [0, 0, 0, 1, 0]\n",
    "apple = [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**Problem:** Every word is the same distance from every other word. There's no notion of similarity.\n",
    "\n",
    "### The Modern Solution: Dense Embeddings\n",
    "\n",
    "An **embedding** is a learned dense vector that captures semantic meaning:\n",
    "\n",
    "```python\n",
    "# These vectors are learned by neural networks\n",
    "cat   = [0.82, -0.13, 0.51, -0.74, 0.23, ...] # 768 numbers\n",
    "dog   = [0.79, -0.09, 0.48, -0.71, 0.19, ...] # Similar to cat!\n",
    "king  = [-0.21, 0.88, 0.05, 0.43, -0.67, ...] \n",
    "queen = [-0.19, 0.85, 0.08, 0.47, -0.65, ...] # Similar to king!\n",
    "apple = [0.34, 0.12, -0.89, 0.45, 0.91, ...] # Different from cat/dog!\n",
    "```\n",
    "\n",
    "**Key Insight:** Words with similar meanings are **close together** in this high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing High-Dimensional Space\n",
    "\n",
    "We can't visualize 768 dimensions, but we can visualize 2D or 3D projections.\n",
    "\n",
    "Let's create a toy embedding space with two semantic clusters:\n",
    "1. **Fruits** (Apple, Banana, Orange, Grape, Pear)\n",
    "2. **Vehicles** (Car, Truck, Bus, Bike, Train)\n",
    "\n",
    "Watch how words with similar meanings cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate fruit embeddings: Centered around [1, 1, 1]\n",
    "fruits = np.random.normal(loc=[1, 1, 1], scale=0.2, size=(5, 3))\n",
    "fruit_labels = [\"Apple\", \"Banana\", \"Orange\", \"Grape\", \"Pear\"]\n",
    "\n",
    "# Generate vehicle embeddings: Centered around [-1, -1, -1]\n",
    "vehicles = np.random.normal(loc=[-1, -1, -1], scale=0.2, size=(5, 3))\n",
    "vehicle_labels = [\"Car\", \"Truck\", \"Bus\", \"Bike\", \"Train\"]\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=fruits[:,0], y=fruits[:,1], z=fruits[:,2],\n",
    "    mode='markers+text',\n",
    "    text=fruit_labels,\n",
    "    textposition=\"top center\",\n",
    "    name='Fruits',\n",
    "    marker=dict(size=10, color='red', opacity=0.8)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=vehicles[:,0], y=vehicles[:,1], z=vehicles[:,2],\n",
    "    mode='markers+text',\n",
    "    text=vehicle_labels,\n",
    "    textposition=\"top center\",\n",
    "    name='Vehicles',\n",
    "    marker=dict(size=10, color='blue', opacity=0.8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Semantic Clusters in 3D Embedding Space\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Dimension 1\",\n",
    "        yaxis_title=\"Dimension 2\",\n",
    "        zaxis_title=\"Dimension 3\"\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n‚úÖ Observation: Fruits cluster together (red), far from Vehicles (blue).\")\n",
    "print(\"This spatial separation is how the model 'knows' an Apple is not a Car.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Thought Exercise\n",
    "\n",
    "**Question:** We visualized 3D space above. Real embeddings (like in GPT or BERT) use 768 or 1536 dimensions. Why do we need so many dimensions?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** More dimensions = more capacity to encode different types of meaning.\n",
    "\n",
    "Think about what embeddings need to capture:\n",
    "- **Synonyms**: happy ‚âà joyful\n",
    "- **Antonyms**: hot ‚â† cold\n",
    "- **Categories**: cat, dog ‚Üí animals\n",
    "- **Grammar**: walk, walked, walking\n",
    "- **Context**: \"bank\" (river) vs \"bank\" (money)\n",
    "- **Analogies**: king - man + woman ‚âà queen\n",
    "\n",
    "In 3D, you can only separate a few concepts cleanly. With 768 dimensions, you can encode thousands of subtle relationships simultaneously!\n",
    "\n",
    "**Analogy**: Think of dimensions as \"channels\" in your brain. More channels = more nuanced understanding.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Distance in High-Dimensional Space\n",
    "\n",
    "If similar words are close together, how do we measure \"closeness\"?\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "The most intuitive measure is **Euclidean distance** (straight-line distance):\n",
    "\n",
    "$$d(\\vec{a}, \\vec{b}) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$$\n",
    "\n",
    "In 2D: $d([1,2], [4,6]) = \\sqrt{(4-1)^2 + (6-2)^2} = \\sqrt{9 + 16} = 5$\n",
    "\n",
    "Let's calculate distances in our toy embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between words\n",
    "apple = fruits[0]\n",
    "banana = fruits[1]\n",
    "car = vehicles[0]\n",
    "\n",
    "# Distance between two fruits (similar words)\n",
    "dist_apple_banana = np.linalg.norm(apple - banana)\n",
    "\n",
    "# Distance between fruit and vehicle (different words)\n",
    "dist_apple_car = np.linalg.norm(apple - car)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Distance Analysis in Embedding Space\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDistance(Apple, Banana): {dist_apple_banana:.4f}\")\n",
    "print(\"  ‚Üí Both are fruits ‚Üí SMALL distance ‚Üí Similar meaning\")\n",
    "\n",
    "print(f\"\\nDistance(Apple, Car): {dist_apple_car:.4f}\")\n",
    "print(\"  ‚Üí Fruit vs Vehicle ‚Üí LARGE distance ‚Üí Different meaning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Key Insight: Semantic similarity = Geometric proximity\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Curse and Blessing of Dimensionality\n",
    "\n",
    "### The Blessing\n",
    "High-dimensional spaces give us:\n",
    "- **More room** to separate concepts\n",
    "- **More capacity** to encode complex relationships\n",
    "- **More expressiveness** for subtle meanings\n",
    "\n",
    "### The Curse\n",
    "But there's a catch: in very high dimensions, **everything is far from everything else**. Distances become less meaningful.\n",
    "\n",
    "**Solution:** Instead of just measuring distance, we also measure **direction** (which we'll learn about in the next lesson using dot products).\n",
    "\n",
    "This is why modern AI uses **both**:\n",
    "- Distance (how far apart?)\n",
    "- Direction (do they point the same way?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. How Are Embeddings Created?\n",
    "\n",
    "You might wonder: where do these magic vectors come from?\n",
    "\n",
    "### Training Process (Simplified)\n",
    "\n",
    "Embeddings are **learned** by neural networks using one of these approaches:\n",
    "\n",
    "1. **Word2Vec** (2013): Predict nearby words\n",
    "   - \"The cat sat on the ___\" ‚Üí predict \"mat\"\n",
    "   - Words that appear in similar contexts get similar vectors\n",
    "\n",
    "2. **Transformer Models** (2017+): Learn from massive text\n",
    "   - GPT, BERT, etc. learn embeddings as part of their architecture\n",
    "   - Updated continuously during training\n",
    "\n",
    "### The Key Principle\n",
    "\n",
    "> **\"You shall know a word by the company it keeps.\"** ‚Äî J.R. Firth, 1957\n",
    "\n",
    "Words that appear in similar **contexts** (nearby words, similar sentences) end up with similar **vectors**.\n",
    "\n",
    "### üìö Recommended Resources\n",
    "\n",
    "To understand how embeddings are trained:\n",
    "- [Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/) by Jay Alammar\n",
    "- [3Blue1Brown: Vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs) ‚Äî Visual introduction to vectors\n",
    "- [TensorFlow Embedding Projector](https://projector.tensorflow.org/) ‚Äî Explore real embeddings interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Knowledge Check Quiz\n",
    "\n",
    "Test your understanding before the programming assignment.\n",
    "\n",
    "### Question 1\n",
    "What is the main advantage of representing words as dense vectors (embeddings) instead of one-hot vectors?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** Dense embeddings capture **semantic relationships** through geometric proximity. Words with similar meanings are close together in vector space, whereas one-hot vectors treat all words as equally different (all pairs are the same distance apart).\n",
    "</details>\n",
    "\n",
    "### Question 2\n",
    "If a word embedding has 768 dimensions, what does that number represent?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** 768 dimensions means each word is represented by a vector with 768 numbers. Each dimension can be thought of as capturing a different aspect of meaning (though they're not interpretable individually). More dimensions = more capacity to encode complex semantic relationships.\n",
    "</details>\n",
    "\n",
    "### Question 3\n",
    "In our fruit/vehicle visualization, why do fruits cluster together?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** Fruits cluster together because they share semantic properties (edible, grows on plants, sweet, etc.). The embedding space learned to position semantically similar concepts near each other. In a real trained model, this happens automatically through exposure to text where fruits appear in similar contexts.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Programming Assignment\n",
    "\n",
    "Now it's your turn to work with embedding spaces.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "In a real Transformer model, embeddings are stored in a matrix:\n",
    "- Shape: `(vocab_size, d_model)`\n",
    "- `vocab_size` = number of unique words (e.g., 50,000)\n",
    "- `d_model` = embedding dimension (e.g., 768)\n",
    "\n",
    "Each **row** is one word's embedding vector.\n",
    "\n",
    "### Your Tasks\n",
    "\n",
    "1. Create a random embedding matrix `E` of shape `(100, 16)`\n",
    "2. Extract embedding vectors for three different \"words\" (indices)\n",
    "3. Calculate the Euclidean distance between pairs of words\n",
    "4. Calculate the magnitude (L2 norm) of a vector\n",
    "5. Identify which pair of words is most similar (smallest distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embedding_assignment():\n",
    "    \"\"\"\n",
    "    Complete this function to work with embedding vectors and vector spaces.\n",
    "    \n",
    "    Returns:\n",
    "        E: Embedding matrix of shape (100, 16)\n",
    "        word1: Embedding vector for word at index 42\n",
    "        word2: Embedding vector for word at index 17\n",
    "        word3: Embedding vector for word at index 89\n",
    "        dist_12: Euclidean distance between word1 and word2\n",
    "        dist_13: Euclidean distance between word1 and word3\n",
    "        dist_23: Euclidean distance between word2 and word3\n",
    "        magnitude: L2 norm of word1\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 1. TODO: Create embedding matrix E of shape (100, 16)\n",
    "    # Use np.random.randn to create random embeddings\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    E = None  # Replace this\n",
    "    \n",
    "    # 2. TODO: Extract embedding vectors for three words\n",
    "    # Get the rows at indices 42, 17, and 89\n",
    "    word1 = None  # index 42\n",
    "    word2 = None  # index 17\n",
    "    word3 = None  # index 89\n",
    "    \n",
    "    # 3. TODO: Calculate Euclidean distances between word pairs\n",
    "    # Hint: Use np.linalg.norm(vec_a - vec_b)\n",
    "    # or calculate manually: sqrt(sum((a - b)^2))\n",
    "    dist_12 = None  # Distance between word1 and word2\n",
    "    dist_13 = None  # Distance between word1 and word3\n",
    "    dist_23 = None  # Distance between word2 and word3\n",
    "    \n",
    "    # 4. TODO: Calculate the L2 norm (magnitude) of word1\n",
    "    # Hint: Use np.linalg.norm(word1)\n",
    "    # or calculate manually: sqrt(sum(x^2))\n",
    "    magnitude = None  # Replace this\n",
    "    \n",
    "    return E, word1, word2, word3, dist_12, dist_13, dist_23, magnitude\n",
    "\n",
    "# Run your implementation\n",
    "try:\n",
    "    E, w1, w2, w3, d12, d13, d23, mag = embedding_assignment()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Assignment Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Embedding matrix shape: {E.shape if E is not None else 'Not implemented'}\")\n",
    "    print(f\"\\nWord 1 (first 5 dims): {w1[:5] if w1 is not None else 'Not implemented'}\")\n",
    "    print(f\"Word 2 (first 5 dims): {w2[:5] if w2 is not None else 'Not implemented'}\")\n",
    "    print(f\"Word 3 (first 5 dims): {w3[:5] if w3 is not None else 'Not implemented'}\")\n",
    "    print(f\"\\nDistance(word1, word2): {d12}\")\n",
    "    print(f\"Distance(word1, word3): {d13}\")\n",
    "    print(f\"Distance(word2, word3): {d23}\")\n",
    "    print(f\"\\nMagnitude of word1: {mag}\")\n",
    "    \n",
    "    if all(x is not None for x in [d12, d13, d23]):\n",
    "        min_dist = min(d12, d13, d23)\n",
    "        if min_dist == d12:\n",
    "            print(\"\\n‚úÖ Most similar pair: word1 and word2\")\n",
    "        elif min_dist == d13:\n",
    "            print(\"\\n‚úÖ Most similar pair: word1 and word3\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ Most similar pair: word2 and word3\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure to replace all None values with your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Tests\n",
    "\n",
    "Run these tests before submitting to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding_assignment():\n",
    "    \"\"\"\n",
    "    Local tests for the embedding assignment.\n",
    "    \"\"\"\n",
    "    print(\"Running local tests...\\n\")\n",
    "    \n",
    "    try:\n",
    "        E, w1, w2, w3, d12, d13, d23, mag = embedding_assignment()\n",
    "        \n",
    "        passed = 0\n",
    "        total = 8\n",
    "        \n",
    "        # Test 1: Embedding matrix shape\n",
    "        if E is not None and E.shape == (100, 16):\n",
    "            print(\"‚úÖ Test 1 PASSED: Embedding matrix has correct shape (100, 16)\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Test 1 FAILED: Expected shape (100, 16), got {E.shape if E is not None else 'None'}\")\n",
    "        \n",
    "        # Test 2-4: Vector shapes\n",
    "        for i, (word, name) in enumerate([(w1, \"word1\"), (w2, \"word2\"), (w3, \"word3\")], start=2):\n",
    "            if word is not None and word.shape == (16,):\n",
    "                print(f\"‚úÖ Test {i} PASSED: {name} has correct shape (16,)\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Test {i} FAILED: {name} expected shape (16,), got {word.shape if word is not None else 'None'}\")\n",
    "        \n",
    "        # Test 5-7: Distance calculations\n",
    "        if all(x is not None for x in [w1, w2, w3]):\n",
    "            expected_d12 = np.linalg.norm(w1 - w2)\n",
    "            expected_d13 = np.linalg.norm(w1 - w3)\n",
    "            expected_d23 = np.linalg.norm(w2 - w3)\n",
    "            \n",
    "            for i, (dist, expected, name) in enumerate([\n",
    "                (d12, expected_d12, \"dist_12\"),\n",
    "                (d13, expected_d13, \"dist_13\"),\n",
    "                (d23, expected_d23, \"dist_23\")\n",
    "            ], start=5):\n",
    "                if dist is not None and np.isclose(dist, expected):\n",
    "                    print(f\"‚úÖ Test {i} PASSED: {name} calculated correctly\")\n",
    "                    passed += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Test {i} FAILED: {name} incorrect\")\n",
    "        else:\n",
    "            print(\"‚ùå Tests 5-7 SKIPPED: Vectors not extracted\")\n",
    "        \n",
    "        # Test 8: Magnitude\n",
    "        if mag is not None and w1 is not None:\n",
    "            expected_mag = np.linalg.norm(w1)\n",
    "            if np.isclose(mag, expected_mag):\n",
    "                print(\"‚úÖ Test 8 PASSED: Magnitude calculated correctly\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(\"‚ùå Test 8 FAILED: Magnitude incorrect\")\n",
    "        else:\n",
    "            print(\"‚ùå Test 8 FAILED: Magnitude not calculated\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Score: {passed}/{total} tests passed\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"\\nüéâ All tests passed! Ready to submit to the server.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Some tests failed. Review your implementation.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running tests: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the tests\n",
    "test_embedding_assignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Submission Instructions\n",
    "\n",
    "Once you pass all local tests:\n",
    "\n",
    "1. **Submit your code** to the API server (instructions provided separately)\n",
    "2. **Server runs hidden tests** to validate your implementation\n",
    "3. **Receive your completion key** if all tests pass\n",
    "4. **Save the key** ‚Äî you'll need all 4 module keys for your certificate\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Lesson 2**, you'll learn about:\n",
    "- **The Dot Product**: Measuring similarity between vectors\n",
    "- How direction (not just distance) encodes semantic relationships\n",
    "- The connection to the attention mechanism\n",
    "\n",
    "This will answer the question: *How do we know which words should \"pay attention\" to each other?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Vectors** are lists of numbers that can represent points in space\n",
    "2. **Vector spaces** are mathematical \"universes\" where vectors live\n",
    "3. **High-dimensional spaces** (768D, 1536D) give us capacity to encode complex semantic relationships\n",
    "4. **Embeddings** transform words into vectors where semantic similarity = geometric proximity\n",
    "5. **Euclidean distance** measures how far apart two vectors are\n",
    "6. Embeddings are **learned** by neural networks from massive text data\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "You've taken the first step toward understanding attention:\n",
    "\n",
    "```\n",
    "Words ‚Üí Vectors ‚Üí Geometric Space ‚Üí Similarity Measures ‚Üí Attention\n",
    "  ^                                                          ^\n",
    " (This lesson)                                      (Module 4)\n",
    "```\n",
    "\n",
    "### Connection to Attention\n",
    "\n",
    "In the attention mechanism, we need to know **which words are related**. By representing words as vectors:\n",
    "- We can **measure** semantic relationships mathematically\n",
    "- We can **compute** which words should attend to each other\n",
    "- We can **learn** these representations from data\n",
    "\n",
    "Next lesson: How to measure similarity using the **dot product**!\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/) ‚Äî How embeddings are trained\n",
    "- [3Blue1Brown: Vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs) ‚Äî Visual introduction to linear algebra\n",
    "- [TensorFlow Embedding Projector](https://projector.tensorflow.org/) ‚Äî Explore real embeddings in 3D\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Module 2, Lesson 2 ‚Äî The Dot Product: Measuring Similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
