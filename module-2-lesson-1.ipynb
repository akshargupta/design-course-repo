{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Lesson 1: Vectors as Meaning\n",
    "\n",
    "**Course:** Foundations of Attention  \n",
    "**Module:** The Geometry of Intelligence  \n",
    "**Learning Objectives:**\n",
    "1. Define high-dimensional vector spaces and their relevance to semantic meaning (Concept)\n",
    "2. Calculate the Dot Product to measure directional alignment (Skill)\n",
    "3. Interpret the Dot Product as a metric for similarity/attention (Concept)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Why Geometry Matters for AI\n",
    "\n",
    "Before we can build attention mechanisms, we need to understand a fundamental question:\n",
    "\n",
    "**How do computers measure similarity between words?**\n",
    "\n",
    "The answer lies in **geometry**. By representing words as vectors in high-dimensional space, we transform the problem of \"meaning\" into a problem of \"distance\" and \"direction.\"\n",
    "\n",
    "This lesson will teach you the mathematical foundation that makes modern AI possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Strings to Numbers: The Embedding Revolution\n",
    "\n",
    "### The Problem\n",
    "Computers cannot understand language. If you feed the string `\"cat\"` into a neural network, it sees nothing but a stream of ASCII bytes:\n",
    "\n",
    "```\n",
    "\"cat\" = [99, 97, 116]  # ASCII values\n",
    "```\n",
    "\n",
    "To perform mathematical operations on language (like computing attention), we must first convert these discrete symbols into continuous numbers.\n",
    "\n",
    "### The Old Way: One-Hot Encoding\n",
    "\n",
    "Historically, we represented words as sparse vectors with a single `1`:\n",
    "\n",
    "```\n",
    "Cat:    [1, 0, 0, 0, 0]\n",
    "Dog:    [0, 1, 0, 0, 0]\n",
    "King:   [0, 0, 1, 0, 0]\n",
    "Queen:  [0, 0, 0, 1, 0]\n",
    "Apple:  [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**The Problem:** These vectors are **orthogonal** (90 degrees apart). The dot product between any two words is 0, which mathematically implies they share *zero* similarity. This is obviously wrong‚Äî\"Cat\" and \"Dog\" are both animals!\n",
    "\n",
    "### The Modern Way: Dense Embeddings\n",
    "\n",
    "An **embedding** is a dense vector of floating-point numbers that represents the *semantic meaning* of a word:\n",
    "\n",
    "```python\n",
    "Cat:   [0.82, -0.13, 0.51, -0.74, ...] # 768 dimensions\n",
    "Dog:   [0.79, -0.09, 0.48, -0.71, ...] # Similar to Cat!\n",
    "King:  [-0.21, 0.88, 0.05, 0.43, ...]\n",
    "Queen: [-0.19, 0.85, 0.08, 0.47, ...] # Similar to King!\n",
    "```\n",
    "\n",
    "In this high-dimensional space:\n",
    "- Words with **similar meanings** point in **similar directions**\n",
    "- The **distance** between vectors encodes **semantic difference**\n",
    "- The **dot product** measures **semantic similarity**\n",
    "\n",
    "This is the foundation of every modern language model, including GPT, BERT, and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization: The Geometry of Semantics\n",
    "\n",
    "Let's visualize how embeddings create semantic clusters. We'll generate two groups of words:\n",
    "\n",
    "1. **Fruits:** (Apple, Banana, Orange, Grape, Pear)\n",
    "2. **Vehicles:** (Car, Truck, Bus, Bike, Train)\n",
    "\n",
    "Watch how words with similar meanings cluster together in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate fruit vectors: Centered around [1, 1, 1]\n",
    "fruits = np.random.normal(loc=[1, 1, 1], scale=0.2, size=(5, 3))\n",
    "fruit_labels = [\"Apple\", \"Banana\", \"Orange\", \"Grape\", \"Pear\"]\n",
    "\n",
    "# Generate vehicle vectors: Centered around [-1, -1, -1]\n",
    "vehicles = np.random.normal(loc=[-1, -1, -1], scale=0.2, size=(5, 3))\n",
    "vehicle_labels = [\"Car\", \"Truck\", \"Bus\", \"Bike\", \"Train\"]\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=fruits[:,0], y=fruits[:,1], z=fruits[:,2],\n",
    "    mode='markers+text',\n",
    "    text=fruit_labels,\n",
    "    textposition=\"top center\",\n",
    "    name='Fruits',\n",
    "    marker=dict(size=10, color='red', opacity=0.8)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=vehicles[:,0], y=vehicles[:,1], z=vehicles[:,2],\n",
    "    mode='markers+text',\n",
    "    text=vehicle_labels,\n",
    "    textposition=\"top center\",\n",
    "    name='Vehicles',\n",
    "    marker=dict(size=10, color='blue', opacity=0.8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Semantic Clusters in 3D Embedding Space\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Dimension 1\",\n",
    "        yaxis_title=\"Dimension 2\",\n",
    "        zaxis_title=\"Dimension 3\"\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Notice how Fruits cluster together, far from Vehicles.\")\n",
    "print(\"This spatial distance is how the model knows that an Apple is not a Car.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Thought Exercise\n",
    "\n",
    "**Question:** In the visualization above, we only used 3 dimensions for display. Real word embeddings (like in GPT or BERT) use 768 or even 1536 dimensions. Why?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** Higher dimensions allow the model to capture more subtle relationships. In 3D, we can only separate a few concepts. But with 768 dimensions, we can encode:\n",
    "- Synonyms (happy ‚âà joyful)\n",
    "- Antonyms (hot ‚â† cold)\n",
    "- Analogies (king - man + woman ‚âà queen)\n",
    "- Grammar (singular vs plural)\n",
    "- Context (\"bank\" in \"river bank\" vs \"savings bank\")\n",
    "\n",
    "Think of it like this: In 2D, you can only draw a few non-overlapping circles. In 768D, you can fit billions of separate concepts!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Dot Product: Measuring Similarity\n",
    "\n",
    "Now comes the critical question: **How do we measure if two words are similar?**\n",
    "\n",
    "The answer is the **dot product** (also called the **inner product**). This is the *single most important operation* in attention mechanisms.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given two vectors $\\vec{a}$ and $\\vec{b}$:\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i \\times b_i = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n$$\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "\n",
    "a ¬∑ b = (1√ó4) + (2√ó5) + (3√ó6) = 4 + 10 + 18 = 32\n",
    "```\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "The dot product can also be written as:\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = ||\\vec{a}|| \\, ||\\vec{b}|| \\, \\cos(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $||\\vec{a}||$ is the magnitude (length) of vector $\\vec{a}$\n",
    "- $\\theta$ is the angle between the vectors\n",
    "\n",
    "**Key Insight:**\n",
    "- If vectors point in the **same direction** ($\\theta = 0¬∞$), then $\\cos(\\theta) = 1$ ‚Üí **high dot product** ‚Üí **high similarity**\n",
    "- If vectors are **perpendicular** ($\\theta = 90¬∞$), then $\\cos(\\theta) = 0$ ‚Üí **zero dot product** ‚Üí **no similarity**\n",
    "- If vectors point in **opposite directions** ($\\theta = 180¬∞$), then $\\cos(\\theta) = -1$ ‚Üí **negative dot product** ‚Üí **opposite meaning**\n",
    "\n",
    "This is exactly how attention works: words with high dot products \"attend\" to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Interactive Example: Dot Product vs Angle\n",
    "\n",
    "Let's calculate the dot product between pairs of words from our semantic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our fruit and vehicle vectors from before\n",
    "apple_vec = fruits[0]  # Apple\n",
    "banana_vec = fruits[1]  # Banana\n",
    "car_vec = vehicles[0]   # Car\n",
    "\n",
    "# Calculate dot products\n",
    "apple_banana_dot = np.dot(apple_vec, banana_vec)\n",
    "apple_car_dot = np.dot(apple_vec, car_vec)\n",
    "\n",
    "# Calculate magnitudes\n",
    "apple_mag = np.linalg.norm(apple_vec)\n",
    "banana_mag = np.linalg.norm(banana_vec)\n",
    "car_mag = np.linalg.norm(car_vec)\n",
    "\n",
    "# Calculate angles (in degrees)\n",
    "angle_apple_banana = np.arccos(apple_banana_dot / (apple_mag * banana_mag)) * 180 / np.pi\n",
    "angle_apple_car = np.arccos(apple_car_dot / (apple_mag * car_mag)) * 180 / np.pi\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Dot Product Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nApple ¬∑ Banana = {apple_banana_dot:.4f}\")\n",
    "print(f\"Angle between Apple and Banana: {angle_apple_banana:.2f}¬∞\")\n",
    "print(\"‚úÖ Both are fruits ‚Üí Similar meaning ‚Üí Small angle ‚Üí HIGH dot product\")\n",
    "\n",
    "print(f\"\\nApple ¬∑ Car = {apple_car_dot:.4f}\")\n",
    "print(f\"Angle between Apple and Car: {angle_apple_car:.2f}¬∞\")\n",
    "print(\"‚úÖ Fruit vs Vehicle ‚Üí Different meaning ‚Üí Large angle ‚Üí LOW (or negative) dot product\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Key Takeaway:\")\n",
    "print(\"The dot product quantifies semantic similarity!\")\n",
    "print(\"This is the foundation of the attention mechanism.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Arithmetic: Encoding Relationships\n",
    "\n",
    "Because embeddings are just numbers, we can do arithmetic on them. The most famous example is:\n",
    "\n",
    "$$\\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}}$$\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Think of it geometrically:\n",
    "- The vector $\\vec{\\text{King}} - \\vec{\\text{Man}}$ represents the \"direction\" from Man to King\n",
    "- This direction encodes the concept of **\"Royalty\"**\n",
    "- Adding that same royalty direction to $\\vec{\\text{Woman}}$ should land you on $\\vec{\\text{Queen}}$!\n",
    "\n",
    "Let's simulate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate word vectors (in reality, these come from trained models)\n",
    "np.random.seed(100)\n",
    "\n",
    "# Create vectors where the relationship holds approximately\n",
    "man = np.array([0.5, 0.1, 0.3])\n",
    "woman = np.array([0.5, 0.9, 0.3])  # Similar to man, but different in one dimension\n",
    "king = np.array([0.9, 0.1, 0.8])   # Similar to man in gender dimension, different in royalty\n",
    "queen = np.array([0.9, 0.9, 0.8])  # Similar to woman in gender, similar to king in royalty\n",
    "\n",
    "# Perform the arithmetic\n",
    "result = king - man + woman\n",
    "\n",
    "print(\"Vector Arithmetic Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"King:   {king}\")\n",
    "print(f\"Man:    {man}\")\n",
    "print(f\"Woman:  {woman}\")\n",
    "print(f\"\\nKing - Man + Woman = {result}\")\n",
    "print(f\"Queen (actual):      {queen}\")\n",
    "print(f\"\\nDistance from result to Queen: {np.linalg.norm(result - queen):.4f}\")\n",
    "print(\"\\n‚úÖ The result is very close to Queen!\")\n",
    "print(\"\\nüéØ This shows embeddings encode semantic relationships as geometric directions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Recommended Resources\n",
    "\n",
    "To dive deeper into word embeddings:\n",
    "- [Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/) by Jay Alammar ‚Äî A beautiful visual guide to how embeddings are trained\n",
    "- [3Blue1Brown: Vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs) ‚Äî The essence of linear algebra, visually explained\n",
    "- [TensorFlow Embedding Projector](https://projector.tensorflow.org/) ‚Äî Explore real embeddings in an interactive 3D space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Knowledge Check Quiz\n",
    "\n",
    "Test your understanding before moving to the programming assignment.\n",
    "\n",
    "### Question 1: Conceptual\n",
    "If two words have very similar meanings (e.g., \"Happy\" and \"Joyful\"), what should be true about their embedding vectors?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** They should point in roughly the same direction, meaning:\n",
    "1. Their **dot product** should be large (close to max value)\n",
    "2. The **angle between them** should be small (close to 0¬∞)\n",
    "3. Their **Euclidean distance** (L2 norm) should be small\n",
    "\n",
    "This is the geometric signature of semantic similarity!\n",
    "</details>\n",
    "\n",
    "### Question 2: Mathematical\n",
    "Given vectors $\\vec{a} = [2, 3, 1]$ and $\\vec{b} = [1, 0, 4]$, calculate their dot product.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** \n",
    "$$\\vec{a} \\cdot \\vec{b} = (2 \\times 1) + (3 \\times 0) + (1 \\times 4) = 2 + 0 + 4 = 6$$\n",
    "</details>\n",
    "\n",
    "### Question 3: Attention Preview\n",
    "In the attention mechanism, we compute dot products between a \"Query\" vector and multiple \"Key\" vectors. What do you think a high dot product represents?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer:** A high dot product between a Query and a Key means those two words/tokens should \"pay attention\" to each other. They are semantically relevant!\n",
    "\n",
    "This is the core insight: **Attention = Dot Product Similarity**\n",
    "\n",
    "In Module 4, you'll implement the full formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The $QK^T$ part is computing all dot products between queries and keys!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Programming Assignment\n",
    "\n",
    "Now it's time to implement what you've learned. This assignment builds toward the final project where you'll implement the full attention mechanism.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "In a real Transformer, the embedding layer is a matrix of shape `(vocab_size, d_model)` where:\n",
    "- `vocab_size` = number of unique words (e.g., 50,000)\n",
    "- `d_model` = embedding dimension (e.g., 768)\n",
    "\n",
    "Each row is a word's embedding vector.\n",
    "\n",
    "### Your Tasks\n",
    "\n",
    "1. Create a random embedding matrix `E` of shape `(100, 16)`\n",
    "2. Extract vectors for two different words\n",
    "3. Calculate the dot product between them\n",
    "4. Calculate the L2 norm (magnitude) of each vector\n",
    "5. Calculate the cosine similarity\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embedding_assignment():\n",
    "    \"\"\"\n",
    "    Complete this function to implement basic embedding operations.\n",
    "    \n",
    "    Returns:\n",
    "        E: Embedding matrix of shape (100, 16)\n",
    "        vec1: Embedding vector for word at index 42\n",
    "        vec2: Embedding vector for word at index 17\n",
    "        dot_product: Dot product between vec1 and vec2\n",
    "        norm1: L2 norm of vec1\n",
    "        norm2: L2 norm of vec2\n",
    "        cosine_sim: Cosine similarity between vec1 and vec2\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 1. TODO: Create a random embedding matrix E using np.random.randn\n",
    "    # Shape should be (100, 16) representing 100 words with 16-dimensional embeddings\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    E = None  # Replace with your implementation\n",
    "    \n",
    "    # 2. TODO: Extract embedding vectors for words at indices 42 and 17\n",
    "    vec1 = None  # Replace with your implementation\n",
    "    vec2 = None  # Replace with your implementation\n",
    "    \n",
    "    # 3. TODO: Calculate the dot product between vec1 and vec2\n",
    "    # Hint: Use np.dot() or the @ operator\n",
    "    dot_product = None  # Replace with your implementation\n",
    "    \n",
    "    # 4. TODO: Calculate the L2 norm (magnitude) of each vector\n",
    "    # Hint: Use np.linalg.norm() or calculate manually: sqrt(sum(x^2))\n",
    "    norm1 = None  # Replace with your implementation\n",
    "    norm2 = None  # Replace with your implementation\n",
    "    \n",
    "    # 5. TODO: Calculate cosine similarity\n",
    "    # Formula: cosine_sim = (vec1 ¬∑ vec2) / (||vec1|| * ||vec2||)\n",
    "    # This normalizes the dot product to be between -1 and 1\n",
    "    cosine_sim = None  # Replace with your implementation\n",
    "    \n",
    "    return E, vec1, vec2, dot_product, norm1, norm2, cosine_sim\n",
    "\n",
    "# Run your implementation\n",
    "try:\n",
    "    E, vec1, vec2, dot_prod, norm1, norm2, cos_sim = embedding_assignment()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Assignment Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Embedding matrix shape: {E.shape if E is not None else 'Not implemented'}\")\n",
    "    print(f\"\\nVector 1 (first 5 dims): {vec1[:5] if vec1 is not None else 'Not implemented'}\")\n",
    "    print(f\"Vector 2 (first 5 dims): {vec2[:5] if vec2 is not None else 'Not implemented'}\")\n",
    "    print(f\"\\nDot product: {dot_prod}\")\n",
    "    print(f\"Norm of vector 1: {norm1}\")\n",
    "    print(f\"Norm of vector 2: {norm2}\")\n",
    "    print(f\"Cosine similarity: {cos_sim}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure to replace all None values with your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Tests (Run These Before Submission)\n",
    "\n",
    "These tests check your implementation locally. You should pass all of these before submitting to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding_assignment():\n",
    "    \"\"\"\n",
    "    Local tests for the embedding assignment.\n",
    "    Run this to verify your implementation before submitting to the server.\n",
    "    \"\"\"\n",
    "    print(\"Running local tests...\\n\")\n",
    "    \n",
    "    try:\n",
    "        E, vec1, vec2, dot_prod, norm1, norm2, cos_sim = embedding_assignment()\n",
    "        \n",
    "        passed = 0\n",
    "        total = 7\n",
    "        \n",
    "        # Test 1: Shape of embedding matrix\n",
    "        if E is not None and E.shape == (100, 16):\n",
    "            print(\"‚úÖ Test 1 PASSED: Embedding matrix has correct shape (100, 16)\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Test 1 FAILED: Expected shape (100, 16), got {E.shape if E is not None else 'None'}\")\n",
    "        \n",
    "        # Test 2: vec1 shape\n",
    "        if vec1 is not None and vec1.shape == (16,):\n",
    "            print(\"‚úÖ Test 2 PASSED: Vector 1 has correct shape (16,)\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Test 2 FAILED: Expected shape (16,), got {vec1.shape if vec1 is not None else 'None'}\")\n",
    "        \n",
    "        # Test 3: vec2 shape\n",
    "        if vec2 is not None and vec2.shape == (16,):\n",
    "            print(\"‚úÖ Test 3 PASSED: Vector 2 has correct shape (16,)\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Test 3 FAILED: Expected shape (16,), got {vec2.shape if vec2 is not None else 'None'}\")\n",
    "        \n",
    "        # Test 4: Dot product correctness\n",
    "        if dot_prod is not None and vec1 is not None and vec2 is not None:\n",
    "            expected_dot = np.dot(vec1, vec2)\n",
    "            if np.isclose(dot_prod, expected_dot):\n",
    "                print(\"‚úÖ Test 4 PASSED: Dot product calculated correctly\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Test 4 FAILED: Dot product mismatch\")\n",
    "        else:\n",
    "            print(\"‚ùå Test 4 FAILED: Dot product not calculated\")\n",
    "        \n",
    "        # Test 5: norm1 correctness\n",
    "        if norm1 is not None and vec1 is not None:\n",
    "            expected_norm = np.linalg.norm(vec1)\n",
    "            if np.isclose(norm1, expected_norm):\n",
    "                print(\"‚úÖ Test 5 PASSED: Norm 1 calculated correctly\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Test 5 FAILED: Norm 1 mismatch\")\n",
    "        else:\n",
    "            print(\"‚ùå Test 5 FAILED: Norm 1 not calculated\")\n",
    "        \n",
    "        # Test 6: norm2 correctness\n",
    "        if norm2 is not None and vec2 is not None:\n",
    "            expected_norm = np.linalg.norm(vec2)\n",
    "            if np.isclose(norm2, expected_norm):\n",
    "                print(\"‚úÖ Test 6 PASSED: Norm 2 calculated correctly\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Test 6 FAILED: Norm 2 mismatch\")\n",
    "        else:\n",
    "            print(\"‚ùå Test 6 FAILED: Norm 2 not calculated\")\n",
    "        \n",
    "        # Test 7: Cosine similarity correctness\n",
    "        if cos_sim is not None and norm1 is not None and norm2 is not None and dot_prod is not None:\n",
    "            expected_cos = dot_prod / (norm1 * norm2)\n",
    "            if np.isclose(cos_sim, expected_cos):\n",
    "                print(\"‚úÖ Test 7 PASSED: Cosine similarity calculated correctly\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Test 7 FAILED: Cosine similarity mismatch\")\n",
    "        else:\n",
    "            print(\"‚ùå Test 7 FAILED: Cosine similarity not calculated\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Score: {passed}/{total} tests passed\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"\\nüéâ All tests passed! You're ready to submit to the server.\")\n",
    "            print(\"\\nNext step: Submit your code to receive your completion key.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Some tests failed. Review your implementation and try again.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running tests: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the tests\n",
    "test_embedding_assignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission Instructions\n",
    "\n",
    "Once you pass all local tests:\n",
    "\n",
    "1. **Review your code** to ensure it follows best practices\n",
    "2. **Submit to the API server** (instructions will be provided separately)\n",
    "3. If all hidden tests pass, you'll **receive a unique completion key**\n",
    "4. **Save your key** ‚Äî you'll need all 4 module keys to generate your certificate\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next lesson, you'll learn about:\n",
    "- **Matrix Multiplication** as linear transformations\n",
    "- How to generate **Query**, **Key**, and **Value** matrices\n",
    "- The geometric interpretation of transforming embedding space\n",
    "\n",
    "These concepts will directly lead to implementing the attention formula in Module 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Embeddings** represent words as dense vectors in high-dimensional space\n",
    "2. **Semantic similarity** is encoded as geometric proximity (distance and direction)\n",
    "3. The **dot product** measures how aligned two vectors are:\n",
    "   - High dot product = Similar meaning\n",
    "   - Low/zero dot product = Unrelated meaning\n",
    "   - Negative dot product = Opposite meaning\n",
    "4. **Vector arithmetic** (e.g., King - Man + Woman ‚âà Queen) works because embeddings encode semantic relationships as geometric directions\n",
    "5. **Cosine similarity** normalizes the dot product to be between -1 and 1\n",
    "\n",
    "### Connection to Attention\n",
    "\n",
    "Everything you learned today builds toward the attention mechanism:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The $QK^T$ term computes dot products between:\n",
    "- **Q** (Query): \"What am I looking for?\"\n",
    "- **K** (Key): \"What information do I have?\"\n",
    "\n",
    "Words with high dot products \"attend\" to each other!\n",
    "\n",
    "### Practice Problems\n",
    "\n",
    "Before moving on, try these challenges:\n",
    "1. Modify the visualization to show 3 semantic clusters instead of 2\n",
    "2. Implement cosine similarity from scratch without using `np.linalg.norm`\n",
    "3. Create a function that finds the top-K most similar words to a given word\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [3Blue1Brown: Dot Products](https://www.youtube.com/watch?v=LyGKycYT2v0) ‚Äî Visual intuition for dot products\n",
    "- [NumPy dot documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) ‚Äî Official docs\n",
    "- [Dot Product in ML (Towards Data Science)](https://towardsdatascience.com/dot-product-in-machine-learning-49e756e8c5a0) ‚Äî ML applications\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Move on to Module 2, Lesson 2: Matrix Multiplication and Linear Transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
